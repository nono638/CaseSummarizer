# Development Log

## Session 13 - GUI Responsiveness Improvements & Critical Issue Discovery (2025-11-28)

**Objective:** Implement UI locking during processing, add cancel button, and resolve GUI unresponsiveness with large documents (260-page PDFs).

### Features Implemented

**1. UI Control Locking During Processing**
- Added `lock_controls()` and `unlock_controls()` methods to `OutputOptionsWidget` (src/ui/widgets.py:309-321)
- Disables slider and checkboxes during processing to prevent mid-flight configuration changes
- Re-enables controls after completion, error, or cancellation

**2. Cancel Processing Button**
- Added red "Cancel Processing" button in Output Options quadrant (src/ui/quadrant_builder.py:156-165)
- Button appears during processing, hidden by default (`grid_remove()`)
- Stops all workers: ProcessingWorker, VocabularyWorker, AI worker (src/ui/main_window.py:275-305)
- Restores UI to editable state on cancellation

**3. Batch Queue Processing for GUI Responsiveness**
- Changed `_process_queue()` from `while True` (unlimited) to batch processing (src/ui/main_window.py:351-384)
- Processes max 10 messages per 100ms cycle to prevent main thread blocking
- Calls `update_idletasks()` after each batch to keep UI responsive
- Prevents "Not Responding" during heavy message loads (260-page PDFs generate hundreds of progress updates)

**4. Configurable Vocabulary Display Limits**
- Added `VOCABULARY_DISPLAY_LIMIT = 150` and `VOCABULARY_DISPLAY_MAX = 500` to config (src/config.py:139-147)
- Based on tkinter Treeview performance research (500+ rows = 40-340 second render times)
- Displays first 150 rows (configurable) with overflow warning label
- Warning: "‚ö† Displaying 150 of 532 terms. 382 more available via 'Save to File' button."
- CSV export saves ALL terms, not just displayed subset (src/ui/dynamic_output.py:268-321)
- Batch insertion (25 rows/batch) with `update_idletasks()` between batches

### Bug Fixes

**1. Missing `Path` Import**
- Fixed `NameError: name 'Path' is not defined` in vocabulary extraction (src/vocabulary/vocabulary_extractor.py:22)
- Added `from pathlib import Path` to imports
- Resolved 5 failing vocabulary tests (now 4/5 pass, 1 fails due to categorization expectation mismatch)

**2. Cancel Button Visibility**
- Added explicit `update_idletasks()` after `grid_remove()` to ensure immediate UI update (src/ui/queue_message_handler.py:174-190)
- Added debug logging to track button state transitions

### Critical Unresolved Issue

**üî¥ Severe GUI Unresponsiveness After Large PDF Processing**
- **Symptoms:** After processing 260-page PDF, GUI becomes extremely slow/unresponsive
- **Observed:** Window dragging lags, view switching (Meta-Summary ‚Üí Rare Word List) freezes, UI sometimes blank
- **Persists:** Even after processing completes and all optimizations applied
- **Hypotheses:** Memory leak, background threads not terminating, UI event queue saturation, Treeview corruption, or Windows-specific issue
- **Status:** Documented in scratchpad.md for next session investigation
- **Next Steps:** Profile memory, verify thread termination, add garbage collection, consider pagination

### Files Modified
- `src/ui/widgets.py` - Added lock/unlock methods to OutputOptionsWidget
- `src/ui/quadrant_builder.py` - Added cancel button
- `src/ui/main_window.py` - Batch queue processing, cancel handler, updated unpacking
- `src/ui/queue_message_handler.py` - Enhanced reset_ui with forced updates and logging
- `src/ui/workers.py` - Added stop event to VocabularyWorker
- `src/ui/workflow_orchestrator.py` - Track vocab_worker for cancellation
- `src/ui/dynamic_output.py` - Configurable display limits, batch insertion, overflow warning
- `src/vocabulary/vocabulary_extractor.py` - Added Path import
- `src/config.py` - Added VOCABULARY_DISPLAY_LIMIT and VOCABULARY_DISPLAY_MAX
- `scratchpad.md` - Documented critical GUI responsiveness issue

### Testing
- ‚úÖ 50/55 tests passing (5 vocabulary tests had Path import issue, now 4/5 pass)
- ‚úÖ UI startup test passed (cancel button exists, lock/unlock methods present)
- ‚ùå Large PDF (260 pages) causes severe GUI unresponsiveness (CRITICAL ISSUE)

---

## Session 12 - Development Log Automatic Condensation Policy (2025-11-28)

**Objective:** Establish automatic condensation policy for development_log.md to prevent token bloat while maintaining useful AI context.

### Problem
Development log grew to 1860 lines with overly detailed old entries (Session 7 was 743 lines alone, 40% of file). Manual condensation reduced to 847 lines, but needed formal policy to prevent recurrence. AI needs recent session details for context, but old entries should be condensed to save tokens.

### Solution
Updated AI_RULES.md with automatic condensation rules using **entry-count thresholds** (not date-based):
- **Most recent 5 sessions:** Full detail (implementation specifics, code examples, testing results)
- **Sessions 6-20:** Condensed to 50-100 lines (keep essentials, remove verbosity)
- **Sessions 21+:** Very condensed (20-30 lines, high-level summary only)
- **Target file size:** <1000 lines total

### Implementation

**1. Updated AI_RULES.md Section 2 (Documentation Ecosystem):**
- Replaced generic "DEV_LOG.md" reference with proper `development_log.md` entry
- Added complete condensation policy with 3-tier entry-count thresholds
- Defined condensation triggers (end-of-session, file size >1200 lines, >25 sessions)

**2. Created AI_RULES.md Section 11 (End-of-Session Documentation Workflow):**
- Defined 4-step workflow: Update development_log.md ‚Üí Update human_summary.md ‚Üí Git operations ‚Üí Confirm to user
- Added Condensation Decision Tree with position-based logic (1-5 detailed, 6-20 condensed, 21+ minimal)
- Included before/after condensation example (743 lines ‚Üí 60 lines)
- Specified verification steps (count sessions, apply condensation, verify file size)

**Files Modified:**
- `AI_RULES.md` - Added 117 lines total (Section 2 update + new Section 11)

### Testing
‚úÖ Policy validated against current development_log.md structure
- Current file: 847 lines (compliant)
- Current sessions: ~12 sessions
- Positions 1-5: Detailed ‚úì (Sessions 11, 10, 9, 8 Part 5, 8 Part 4)
- Positions 6-20: Already condensed ‚úì (Session 7, Historical Summary, etc.)
- No immediate condensation needed

### Impact
Future sessions will automatically maintain optimal log size:
- **Consistent maintenance:** No more ad-hoc "the log is too big" requests
- **Token efficiency:** More room for actual code in context window
- **Better AI context:** Recent detailed entries + historical summaries balance
- **User transparency:** Clear policy documented in AI_RULES.md for both Claude and Gemini

---

## Session 11 - Additional Vocabulary Extraction Improvements (2025-11-27)

**Objective:** Implement 6 additional fixes to vocabulary extraction based on analysis of actual CSV output. Focus on filtering legal boilerplate, deduplication, and law firm detection.

### Issues Fixed

**Fix #1: Legal Citations Filtered**
- **Problem**: Statute references (CPLR SS3043, Education Law SS6527, etc.) appearing in output
- **Solution**: Added `LEGAL_CITATION_PATTERNS` with 4 regex patterns
- **Impact**: ~10-15 useless entries filtered per document

**Fix #2: Legal Boilerplate Filtered**
- **Problem**: Standard legal terminology (Verified Answer, Cause of Action, etc.) extracted as vocabulary
- **Solution**:
  - Added `LEGAL_BOILERPLATE_PATTERNS` (5 phrase patterns)
  - Added 10 boilerplate terms to `config/common_medical_legal.txt`
- **Impact**: ~10-20 useless entries filtered per document

**Fix #3: Case Citations Filtered**
- **Problem**: Case names (Mahr v. Perry pattern) extracted as person names
- **Solution**: Added `CASE_CITATION_PATTERN` to filter "X v. Y" format
- **Impact**: ~1-5 entries filtered per document

**Fix #4: Geographic Codes Filtered**
- **Problem**: ZIP codes (NY 11354) and location codes extracted as places
- **Solution**: Added `GEOGRAPHIC_CODE_PATTERNS` (2 patterns)
- **Impact**: ~2-5 entries filtered per document

**Fix #5: Deduplication Implemented** ‚ú®
- **Problem**: Same entity extracted multiple times with variations:
  - "XIANJUN LIANG" AND "Plaintiff XIANJUN LIANG"
  - "State of New York" AND "the State of New York"
  - Partial names "XIANJUN" when full name "XIANJUN LIANG" exists
- **Solution**: New `_deduplicate_terms()` method with two-pass algorithm:
  1. **Prefix normalization**: Remove "the/a/an" prefixes, party labels
  2. **Substring filtering**: If "XIANJUN LIANG" exists, filter out "XIANJUN"
- **Impact**: ~30-40% duplicate entries removed

**Fix #6: Law Firm Detection**
- **Problem**: Law firms mis-categorized as medical terms or generic places
  - "EDELMAN & DICKER" ‚Üí Medical term
  - "THE JACOB D. FUCHSBERG LAW FIRM" ‚Üí Medical term
- **Solution**: Added 3 law firm detection patterns to `STENOGRAPHER_PLACE_PATTERNS`
- **Impact**: ~5-10 entries now correctly categorized as "Law firm"

### Implementation Details

**Files Modified:**

1. **src/vocabulary/vocabulary_extractor.py** (Major changes)
   - Added 4 pattern constant sets (lines 60-84):
     - `LEGAL_CITATION_PATTERNS` (4 patterns)
     - `LEGAL_BOILERPLATE_PATTERNS` (5 patterns)
     - `CASE_CITATION_PATTERN` (1 pattern)
     - `GEOGRAPHIC_CODE_PATTERNS` (2 patterns)
   - Applied all pattern filters in `_is_unusual()` (lines 496-513)
   - Implemented `_deduplicate_terms()` method (60 lines, lines 750-808)
   - Called deduplication in `extract()` (line 658)

2. **src/vocabulary/role_profiles.py** (Law firm detection)
   - Added 3 law firm patterns to `STENOGRAPHER_PLACE_PATTERNS` (lines 122-125)
   - Patterns detect: "Smith & Jones", "THE...LAW FIRM", "...LLP/PC/PLLC"

3. **config/common_medical_legal.txt** (Blacklist expansion)
   - Added 10 legal boilerplate terms (verified, affirmant, complainant, etc.)

### Expected Impact

**Before Session 11 fixes:**
- Session 10 reduced 506 rows ‚Üí ~100-150 rows

**After Session 11 fixes:**
- Estimated final output: ~50-80 rows
- Breakdown:
  - Legal citations: -15 rows
  - Duplicates: -30 rows (largest impact!)
  - Boilerplate: -15 rows
  - Case citations: -5 rows
  - Geographic codes: -5 rows

**Net result:** Highly focused vocabulary list with minimal noise

### Code Quality Metrics
- **Net change**: 3 files modified
- **Lines added**: ~100 (patterns, deduplication method)
- **Lines removed**: ~0 (all additions)
- **All modules**: Under 900 lines (well under 1500 limit)
- **Compilation**: ‚úÖ All imports successful

### Testing
‚úÖ Import test passed
‚úÖ Threshold verified: 150,000
‚úÖ All pattern constants loaded
‚úÖ Deduplication method compiles

### User Testing Required
1. Process same document that generated problematic CSV
2. Compare before (506 rows) vs after (should be 50-80 rows)
3. Verify deduplication works:
   - No "Plaintiff XIANJUN LIANG" if "XIANJUN LIANG" exists
   - No partial names ("XIANJUN") if full name exists
   - No "the State of New York" if "State of New York" exists
4. Verify filtering works:
   - No legal citations (CPLR SS3043, Education Law SS6527)
   - No boilerplate (Verified Answer, Affirmant)
   - No case citations (Mahr v. Perry)
   - No ZIP codes (NY 11354)
5. Verify law firms correctly categorized

---

## Session 10 - Vocabulary Extraction Bug Fixes & Precision Improvements (2025-11-27)

**Objective:** Fix 5 critical bugs in vocabulary extraction causing common words, mis-categorizations, and fragments in output. Improve filtering precision to provide stenographers with ONLY proper names and unfamiliar medical/technical terms.

### Problems Fixed

**Bug #1: Common Words Leaking Through**
- **Root Cause:** Lines 423-429 in `vocabulary_extractor.py` - NER entities and medical terms bypassed frequency check entirely
- **Symptoms:** "the", "and", "medical", "hospital", "doctor", "plaintiff" appearing in CSV output
- **Fix:** Added frequency check for single-word entities and medical terms. Multi-word entities still bypass (e.g., "John Smith"), but single words must pass rarity threshold.

**Bug #2: Threshold Too Permissive (75K)**
- **Root Cause:** 75,000 rank threshold allowed common words through
  - Rank 501: "medical" (155M occurrences) - FILTERED NOW
  - Rank 1345: "hospital" (85M occurrences) - FILTERED NOW
  - Rank 75,000: "chechens" (164K occurrences) - still common
- **Fix:** Increased threshold from 75,000 ‚Üí 150,000 (filters top 45% of 333K vocabulary)

**Bug #3: Mis-categorization (e.g., "ANDY CHOY" ‚Üí "Medical facility")**
- **Root Cause Chain:**
  1. spaCy tags ALL CAPS names as ORG instead of PERSON
  2. ORG category ‚Üí "Place" type
  3. Regex patterns `[A-Z][a-z]+` don't match ALL CAPS
  4. `detect_place_relevance()` substring matches "CHOY Medical Center"
- **Fix:**
  - Updated ALL person/place regex patterns from `[a-z]+` to `[a-zA-Z]+`
  - Stricter place matching: require preposition context OR 2+ word facility names
  - `_places_match()` now requires 50% token overlap (not substring matching)

**Bug #4: Entity Fragments (e.g., "and/or lung")**
- **Root Cause:** spaCy includes leading/trailing context in `ent.text`
- **Fix:** New `_clean_entity_text()` method removes:
  - Leading/trailing conjunctions ("and/or", "and", "or")
  - Newlines and excess whitespace
  - Leading/trailing punctuation

**Bug #5: Title Abbreviations (e.g., "M.D.", "Ph.D.", "Esq.")**
- **Root Cause:** Acronym regex `[A-Z]{2,}` matches title abbreviations stenographers already know
- **Fix:** New `TITLE_ABBREVIATIONS` set filters 24 common titles before accepting as rare acronym

### Implementation Details

**Files Modified:**

1. **src/config.py** (Line 135)
   - Changed `VOCABULARY_RARITY_THRESHOLD = 75000` ‚Üí `150000`
   - Added documentation: "Words with rank >= 150,000 are considered rare (bottom 55%)"

2. **src/vocabulary/vocabulary_extractor.py** (5 changes)
   - Added `TITLE_ABBREVIATIONS` set (24 common titles) after line 50
   - Added `_clean_entity_text()` method (30 lines) after line 211
   - Updated `_first_pass_extraction()` to use entity cleaning (line 607)
   - **Major fix:** Updated `_is_unusual()` lines 460-491:
     - NER entities: multi-word bypass, single-word must pass frequency check
     - Medical terms: must pass frequency check (filters "hospital", "doctor", etc.)
     - Acronyms: filter title abbreviations before accepting
   - Loaded common words blacklist in `__init__()` (line 126)
   - Added blacklist check in `_is_unusual()` (line 463)

3. **src/vocabulary/role_profiles.py** (3 changes)
   - Updated `STENOGRAPHER_PERSON_PATTERNS` (10 patterns): `[a-z]+` ‚Üí `[a-zA-Z]+`
   - Updated `STENOGRAPHER_PLACE_PATTERNS` for stricter matching:
     - Require preposition context: `(?:at|to|from|near)\s+...Hospital`
     - Require 2+ word names: `([A-Z][a-zA-Z]+\s+[A-Z][a-zA-Z]+)\s+Hospital`
   - Updated `_places_match()` for 50% token overlap requirement

4. **config/common_medical_legal.txt** (NEW FILE)
   - Defense-in-depth blacklist with 65+ common words
   - Common medical: hospital, doctor, physician, patient, treatment, surgery, etc.
   - Common legal: plaintiff, defendant, attorney, lawyer, court, judge, etc.

### Testing

**Compilation Test:** ‚úÖ All modules import successfully
```
Rarity threshold: 150000 ‚úì
StenographerProfile loaded ‚úì
Title abbreviations loaded ‚úì
Blacklist loaded ‚úì
```

**Expected Improvements:**
- ‚ùå "the", "and", "of", "medical", "hospital", "doctor" ‚Üí NOW FILTERED
- ‚ùå "M.D.", "Ph.D.", "Esq.", "R.N." ‚Üí NOW FILTERED
- ‚ùå "and/or lung" fragments ‚Üí NOW CLEANED TO "lung" or FILTERED
- ‚úÖ "ANDY CHOY" ‚Üí NOW Person, not Medical facility
- ‚úÖ "adenocarcinoma", "bronchogenic", "carcinoma" ‚Üí STILL EXTRACTED (rare medical)
- ‚úÖ Multi-word entities ‚Üí STILL BYPASS (e.g., "John Smith", "Memorial Hospital")

### Code Quality Metrics
- **Net change:** 6 files modified, 1 file created
- **Lines added:** ~180 (new methods, constants, comments)
- **Lines removed:** ~10 (replaced old logic)
- **All modules:** Under 750 lines (well under 1500 limit)
- **Backward compatible:** All existing tests should still pass
- **Modular design:** Preserved for future attorney/paralegal profiles

### Pattern Established
**Defense-in-Depth Filtering:** When implementing rarity filters, use multiple layers:
1. Hard exclusions (blacklists for absolute no-gos)
2. Frequency-based filtering (statistical rarity)
3. Semantic filtering (NER, medical terms list)
4. Post-processing (entity cleaning, title filtering)

This pattern applies to any filtering system where false positives are costly.

### User Testing Required
Before marking this complete, user should test with the original problematic document:
1. Generate new vocabulary CSV from same document
2. Verify common words are filtered ("the", "and", "medical", "hospital")
3. Verify ALL CAPS names categorized correctly ("ANDY CHOY" ‚Üí Person)
4. Verify no fragments appear ("and/or lung" should be cleaned/filtered)
5. Verify title abbreviations filtered ("M.D.", "Ph.D.")
6. Compare old CSV (506 rows) vs new CSV (should be <100 rows with only meaningful terms)

---

## Session 9 - Vocabulary Extraction Redesign for Stenographer Workflow (2025-11-27)

**Objective:** Redesign vocabulary CSV output to provide actionable, context-aware information for court reporters preparing for depositions. Replace academic categorization with practical role detection tailored to stenographer needs while maintaining modular architecture for future profession expansion.

### Problem Addressed
The existing vocabulary extraction produced technically correct but practically useless output for stenographers:
- **Academic categories** like "Proper Noun (Person)" don't help stenographers prepare
- **Generic relevance scores** ("High", "Medium", "Low") lack context
- **Dictionary definitions for names/places** waste space (stenographers need WHO/WHY, not what the word means)
- **No context about roles** ‚Äî is "Dr. Martinez" the plaintiff's doctor or defendant's doctor?

### Solution Implemented

**1. Modular Role Detection Architecture (`src/vocabulary/role_profiles.py` - NEW FILE)**
- Created `RoleDetectionProfile` base class for profession-specific relevance detection
- Implemented `StenographerProfile` with pattern-based role/relevance detection
- Enables future expansion: `LawyerProfile`, `ParalegalProfile`, `JournalistProfile` (just 50 lines each)
- Uses dependency injection: `VocabularyExtractor(role_profile=StenographerProfile())`

**2. Simplified Category System**
**Before:** "Proper Noun (Person)", "Proper Noun (Organization)", "Acronym", "Technical Term"
**After:** Person, Place, Medical, Technical

**3. Context-Aware Role/Relevance Detection**

**People Detection Patterns:**
```python
STENOGRAPHER_PERSON_PATTERNS = [
    (r'plaintiff[\'s]?\s+(?:attorney|counsel)?\s*([A-Z]...)', 'Plaintiff attorney'),
    (r'plaintiff\s+([A-Z]...)', 'Plaintiff'),
    (r'treating\s+(?:physician|doctor)\s+([A-Z]...)', 'Treating physician'),
    (r'(?:Dr\.|Doctor)\s+([A-Z]...)', 'Medical professional'),
    (r'witness\s+([A-Z]...)', 'Witness'),
]
```

**Place Detection Patterns:**
```python
STENOGRAPHER_PLACE_PATTERNS = [
    (r'accident\s+(?:at|on|near)\s+([A-Z]...)', 'Accident location'),
    (r'([A-Z]...)\s+Hospital', 'Medical facility'),
    (r'surgery\s+(?:at|performed at)\s+([A-Z]...)', 'Surgery location'),
]
```

**4. Smart Definition Display**
- **Person/Place:** No definition needed (stenographers need WHO/WHY, not dictionary meanings)
- **Medical/Technical:** Provide WordNet definitions for unfamiliar terminology
- Saves CSV space and improves readability

**5. Enhanced Regex Filtering**
Expanded `VARIATION_FILTERS` to catch more word variations:
```python
VARIATION_FILTERS = [
    r'^[a-z]+\(s\)$',          # plaintiff(s), defendant(s)
    r'^[a-z]+s\(s\)$',         # defendants(s) (double plurals)
    r'^[a-z]+\([a-z]+\)$',     # word(variant) (any parenthetical)
    r'^[a-z]+\'s$',            # plaintiff's (possessives)
    r'^[a-z]+-[a-z]+$',        # hyphenated variations
]
```

**6. Optimized Rarity Calculation**
**Before:** O(n) percentile calculation on every word check
**After:** O(1) cached rank lookup
- Sorts 333K word frequency dataset once during `__init__()`
- Builds `frequency_rank_map: Dict[str, int]` for instant lookups
- Massive performance improvement for large documents

### CSV Output Transformation

**Before (Academic):**
```csv
Term,Category,Relevance to Case,Definition
Dr. Sarah Martinez,Proper Noun (Person),High,N/A
Lenox Hill Hospital,Proper Noun (Organization),High,N/A
lumbar discectomy,Technical Term,Medium,removal of disk
```

**After (Stenographer-Focused):**
```csv
Term,Type,Role/Relevance,Definition
Dr. Sarah Martinez,Person,Treating physician,‚Äî
Lenox Hill Hospital,Place,Medical facility,‚Äî
lumbar discectomy,Medical,Medical term,Surgical removal of herniated disc material from lower spine
```

### Technical Changes

**Files Modified:**
1. **`src/vocabulary/role_profiles.py`** (NEW - 280 lines):
   - `RoleDetectionProfile` base class
   - `StenographerProfile` implementation
   - Documented placeholders for future profiles

2. **`src/vocabulary/vocabulary_extractor.py`** (Major refactor):
   - Added `role_profile` parameter to `__init__()` (defaults to `StenographerProfile()`)
   - Built cached frequency rank map in `_load_frequency_dataset()`
   - Simplified `_is_word_rare_enough()` to use O(1) rank lookup
   - Simplified `_get_category()`: Person/Place/Medical/Technical (4 categories, not 7+)
   - Updated `_get_definition()`: Takes `category` parameter, returns "‚Äî" for Person/Place
   - Removed `_calculate_relevance()` ‚Üí replaced with profile-based role detection
   - Updated `_second_pass_processing()`: Now calls `role_profile.detect_person_role()` and `role_profile.detect_place_relevance()`
   - Changed output dict keys: "Category" ‚Üí "Type", "Relevance to Case" ‚Üí "Role/Relevance"

3. **`src/ui/dynamic_output.py`** (Column header updates):
   - Updated Treeview columns: `("Term", "Type", "Role/Relevance", "Definition")`
   - Updated CSV export headers to match
   - Updated data access: `item.get("Type")`, `item.get("Role/Relevance")`
   - Adjusted column widths: Type=120px, Role/Relevance=200px

4. **`tests/test_vocabulary_extractor.py`** (Updated for new API):
   - Updated `test_get_category()`: Expects "Person", "Place", "Technical", "Medical"
   - Updated `test_get_definition()`: Now requires `category` parameter
   - Updated `test_extract()`: Expects "Type" and "Role/Relevance" keys in output
   - All 5 tests passing ‚úÖ

### Code Quality Improvements
- **Net reduction:** 473 insertions, 615 deletions (-142 lines total)
- **Better separation of concerns:** Profession-specific logic isolated in profiles
- **Performance optimization:** Cached rank map eliminates repeated sorting
- **Future-proof design:** Adding new profession = 50 lines of patterns, zero core changes

### Pattern Established
**Role Detection System:** When adding profession-specific behavior, create a new `RoleDetectionProfile` subclass instead of modifying core extraction logic. This pattern applies to future features requiring customizable behavior (e.g., output formatters, filtering strategies).

### Next Steps (User Testing Required)
Before considering this feature complete, user should manually test with real legal documents:
1. Verify regex filters work (no "plaintiff(s)" in output)
2. Verify role detection works ("plaintiff John Smith" shows role "Plaintiff")
3. Verify rarity filtering works (common words excluded)
4. Verify UI displays correctly (new column headers in Treeview)

---

## Session 8 Part 5 - Google Word Frequency Dataset Integration (2025-11-26)

**Objective:** Integrate Google's 333K word frequency dataset to filter out common words from vocabulary extraction, allowing only truly rare terms to be included in the results.

### Problem Addressed
Vocabulary extraction was producing too many false positives: common words like "plaintiff(s)", "defendant(s)", and other variations of legal terminology were being flagged as "rare vocabulary." The existing WordNet filter wasn't granular enough to distinguish between statistically common words and truly unusual domain-specific terms.

### Solution Implemented

**1. Google Word Frequency Dataset Integration:**
- File: `Word_rarity-count_1w.txt` (333,333 words, tab-separated format: `word\tfrequency_count`)
- Loaded into memory as `Dict[str, int]` mapping word ‚Üí frequency count
- Lower count = rarer word (fewer occurrences in Google's corpus)

**2. New Methods in VocabularyExtractor:**
- `_load_frequency_dataset()` ‚Äî Parses tab-separated frequency file (handles missing file gracefully)
- `_matches_variation_filter()` ‚Äî Regex-based filtering for word variations (plaintiff(s), defendant's, hyphenated)
- `_is_word_rare_enough()` ‚Äî Determines if word meets rarity threshold using frequency dataset
- `_sort_by_rarity()` ‚Äî Sorts vocabulary results by rarity (unknown words first, then lowest frequency counts)

**3. Regex Variation Filters:**
```python
VARIATION_FILTERS = [
    r'^[a-z]+\(s\)$',      # Matches "plaintiff(s)", "defendant(s)", etc.
    r'^[a-z]+\'s$',        # Matches possessives like "plaintiff's"
    r'^[a-z]+-[a-z]+$',    # Matches hyphenated variations
]
```
- Extensible: Users can add more patterns later
- Located at top of `vocabulary_extractor.py` for easy maintenance

**4. User-Customizable Configuration:**
- `VOCABULARY_RARITY_THRESHOLD = 75000` ‚Äî Only accept words outside top 75K most common (out of 333K)
- `VOCABULARY_SORT_BY_RARITY = True` ‚Äî Enable/disable rarity-based sorting
- Both configurable in `src/config.py` (no code changes needed)

### Updated Filtering Chain (in `_is_unusual()`)
```
1. Basic checks (alpha, whitespace, punctuation)
2. Legal term exclusions
3. User exclusions
4. ‚ú® NEW: Variation filter (plaintiff(s), defendant's, etc.)
5. Named entities (PERSON, ORG, GPE, LOC) ‚Üí always accept
6. Medical terms ‚Üí always accept
7. Acronyms (2+ uppercase) ‚Üí always accept
8. ‚ú® NEW: Frequency-based rarity (Google dataset)
9. WordNet fallback (not in dictionary = rare)
```

### Sorting Strategy (if enabled)
1. **Words NOT in 333K dataset** (appear first) ‚Äî Rarest of the rare
2. **Words in dataset sorted by frequency count** (ascending) ‚Äî Lowest count = rarest

### Files Modified
- `src/config.py` ‚Äî Added `GOOGLE_WORD_FREQUENCY_FILE`, `VOCABULARY_RARITY_THRESHOLD`, `VOCABULARY_SORT_BY_RARITY`
- `src/vocabulary/vocabulary_extractor.py` ‚Äî Added 4 new methods, updated `_is_unusual()`, updated `extract()` for sorting

### Testing
- ‚úÖ All 55 tests passing (no regressions)
- ‚úÖ Frequency dataset loads successfully (333,333 words)
- ‚úÖ Module imports without errors
- ‚úÖ Backward compatible ‚Äî existing API unchanged

### Design Benefits
1. **Probabilistic + Categorical Filtering:** Frequency dataset (statistical) + WordNet (categorical) = comprehensive word rarity assessment
2. **User-Driven Customization:** Threshold and sorting can be adjusted without code changes
3. **Extensible:** Variation filters can be added anytime by editing regex patterns
4. **Graceful Degradation:** Falls back to WordNet if frequency file missing
5. **Performance:** Sorted-by-rarity CSV helps users quickly find the most unusual vocabulary

### Next Steps
- User testing with real legal documents to verify "plaintiff(s)" and "defendant(s)" are now filtered
- Threshold adjustment if results still include too many common words
- Additional variation patterns added as they're discovered

---

## Session 8 Part 4 - Recursive Length Enforcement (2025-11-26)

**Objective:** Implement recursive summarization to ensure AI-generated summaries meet user's requested word count target.

### Problem Addressed
When users request a 200-word summary, LLMs often produce 300-500 words instead. Simply truncating would lose important information at the end. The solution: recursively condense over-length summaries until they meet the target.

### Implementation

**1. Configuration:**
- **20% tolerance**: A 200-word target accepts up to 240 words before triggering condensation
- **3 max attempts**: After 3 condensation tries, return best effort (prevents infinite loops)
- **Applies to all summaries**: Both individual document summaries and meta-summaries

**2. New Methods in OllamaModelManager:**
- `_enforce_length(summary, target_words, max_attempts)` - Main enforcement loop
- `_condense_summary(summary, target_words)` - Generates condensed version via AI

**3. Condensation Prompt Template:**
- Created `config/prompts/phi-3-mini/_condense-summary.txt`
- Underscore prefix means it's for internal use (not shown in dropdown)
- Instructs AI to preserve key facts while reducing verbosity

### Algorithm Flow
```
1. Generate initial summary
2. Check word count
3. If actual_words > target * 1.2:
   - Call _condense_summary()
   - Check again
   - Repeat up to 3 times
4. Return final summary (within tolerance or best effort)
```

### Files Modified
- `src/ai/ollama_model_manager.py` - Added `_enforce_length()` and `_condense_summary()` methods

### Files Created
- `config/prompts/phi-3-mini/_condense-summary.txt` - Condensation prompt template

### Debug Logging
The implementation logs each step for debugging:
```
[LENGTH ENFORCE] Target: 200 words, Max acceptable: 240 words, Actual: 350 words
[LENGTH ENFORCE] Attempt 1/3: Summary is 350 words (>240). Condensing...
[LENGTH ENFORCE] After condensation: 215 words
[LENGTH ENFORCE] Success: 215 words (within 20% tolerance of 200)
```

### Testing
- ‚úÖ All 55 tests pass
- ‚úÖ Module imports successfully
- ‚è≥ Live testing with Ollama pending (requires document processing)

### Refactoring: Separation of Concerns (Post-Implementation)

After the initial implementation, a review identified that length enforcement logic was tightly coupled to `OllamaModelManager`. This violated separation of concerns: the model manager shouldn't be responsible for post-processing logic.

**Refactoring Solution:**
1. Created new `SummaryPostProcessor` class (`src/ai/summary_post_processor.py`)
2. Moved `_enforce_length()` and `_condense_summary()` methods to new class
3. Updated OllamaModelManager to delegate to SummaryPostProcessor after generation
4. Result: Clean separation between generation (model manager) and post-processing (post-processor)

### Files Created (Post-Refactor)
- `src/ai/summary_post_processor.py` - New dedicated post-processing class

### Status
Phase 8 Part 4 complete. Length enforcement now functional and properly separated into dedicated module.

---

## Session 8 Part 3 - Prompt Selection UI Refinement (2025-11-26)

**Objective:** Polish the prompt selection UI by removing placeholder descriptions and adding comprehensive tooltips with real prompt content preview.

### Changes Made

**1. Removed Placeholder Descriptions**
- Deleted generic descriptions like "A balanced prompt..." from prompt template files
- Files affected: All 6 prompt templates in `config/prompts/phi-3-mini/`
- Rationale: Descriptions were placeholders never shown to user; tooltips provide better UX

**2. Added Comprehensive Tooltips**
- Tooltip shows actual prompt content (first 300 characters)
- Applied to both dropdown and label widgets
- Binding: `<Enter>` shows tooltip, `<Leave>` hides it
- Tooltip positioning: Right-aligned relative to dropdown

**3. Tooltip Implementation**
- Created `_show_prompt_tooltip()` and `_hide_prompt_tooltip()` methods in ModelSelectionWidget
- Tooltip window uses CTkToplevel with wraplength for readability
- Handles edge case: Missing prompt file shows "Prompt file not found"

### Files Modified
- `src/ui/widgets.py` - Added tooltip methods and event bindings
- All 6 prompt templates in `config/prompts/phi-3-mini/` - Removed placeholder descriptions

### Status
Prompt selection UI now provides clear preview of prompt content via hover tooltip.

---

## Session 8 Part 2 - Prompt Selection UI with Persistent User Prompts (2025-11-26)

**Objective:** Allow users to customize prompts by editing template files while ensuring changes persist across code updates via `.gitignore`.

### Implementation

**1. Prompt Template System**
- Created 6 prompt templates in `config/prompts/phi-3-mini/` directory
- Each template is a plain text file with placeholder `{{DOCUMENT_TEXT}}`
- Templates: single-document.txt, meta-summary.txt, 4 custom variations
- Prompt files added to `.gitignore` so user edits won't be overwritten by git updates

**2. UI Integration**
- Added dropdown in Model Selection quadrant with 6 prompt options
- Dropdown reads from `config/prompts/{model_name}/` directory
- Selected prompt sent to OllamaModelManager during summarization

**3. Graceful Fallback**
- If prompt file missing: uses hardcoded fallback prompt
- If prompt directory missing: creates it with default templates on first run
- Ensures application never crashes due to missing prompts

### Files Created
- `config/prompts/phi-3-mini/single-document.txt`
- `config/prompts/phi-3-mini/meta-summary.txt`
- Plus 4 custom prompt variants

### Files Modified
- `src/ui/widgets.py` - Added prompt dropdown to ModelSelectionWidget
- `src/ai/ollama_model_manager.py` - Updated to accept and use selected prompt template
- `.gitignore` - Added `config/prompts/` to prevent overwriting user customizations

### Status
Users can now freely edit prompt templates without fear of losing changes during updates.

---

## Session 8 - System Monitor, Tooltips, and Vocabulary Table Overhaul (2025-11-26)

**Objective:** Implement comprehensive UI improvements: system monitor widget, tooltip system, dynamic vocabulary table display, and related bug fixes.

### Summary
Completed 5 major UI enhancements. Created SystemMonitor widget with real-time CPU/RAM tracking and color-coded thresholds. Implemented tooltip system for all quadrant headers. Built dynamic vocabulary table with live filtering. Fixed file size formatting bug. Improved model dropdown persistence. All 55 tests passing.

### Features Implemented

**1. System Monitor Widget** (`src/ui/system_monitor.py`)
- Real-time CPU and RAM usage display in status bar
- Color thresholds: Green (0-74%), Yellow (75-84%), Orange (85-90%), Red (90%+)
- Hover tooltip shows detailed hardware info (CPU model, cores, frequencies)
- Background daemon thread updates every 1 second

**2. Tooltip System** (`src/ui/widgets.py`)
- Created reusable TooltipMixin class
- Applied to all 4 quadrant header labels
- Advanced user guidance (not beginner-oriented)
- 500ms hover delay prevents flickering

**3. Dynamic Vocabulary Table** (`src/ui/dynamic_output.py`)
- TreeView widget displays vocabulary results
- Columns: Term, Category, Relevance, Definition
- Live filtering: Show All / Rare Only / Unusual Only
- CSV export functionality

**4. Bug Fixes**
- File size rounding: Unified to integers across all units (KB, MB, GB)
- Model dropdown: Preserves user selection across refreshes

### Files Created
- `src/ui/system_monitor.py` (230 lines)
- `src/ui/dynamic_output.py` (180 lines)

### Files Modified
- `src/ui/widgets.py` - Tooltip system, file size fix, model dropdown fix
- `src/ui/main_window.py` - Integrated all new components

### Testing
‚úÖ All 55 tests passing
‚úÖ System monitor refreshes correctly
‚úÖ Tooltips appear/disappear without flicker
‚úÖ Vocabulary table displays results
‚úÖ File sizes format consistently

### Status
Session 8 complete. UI now professional-grade with comprehensive real-time feedback.

---

## Session 7 - Separation of Concerns Refactoring (2025-11-26)

**Objective:** Comprehensive code review and refactoring to improve separation of concerns, eliminate code duplication, and consolidate dual logging systems.

### Summary
Performed full codebase review identifying 5 separation-of-concerns issues and implemented all fixes. Created `WorkflowOrchestrator` class to separate business logic from UI message handling. Consolidated dual logging systems into unified `logging_config.py`. Moved `VocabularyExtractor` to its own package. Created shared text utilities. All 55 tests passing after refactoring.

### Problems Addressed & Solutions

**Issue #1: VocabularyExtractor Location** - Created `src/vocabulary/` package and moved extractor there for consistency with other pipeline components.

**Issue #2: QueueMessageHandler Had Business Logic** - Created `WorkflowOrchestrator` class to separate workflow decisions from UI message routing. QueueMessageHandler now purely handles message dispatch and UI updates.

**Issue #3: Hardcoded Config Paths** - Replaced hardcoded paths with config imports (`LEGAL_EXCLUDE_LIST_PATH`, `MEDICAL_TERMS_LIST_PATH`).

**Issue #4: Dual Logging Systems** - Consolidated `debug_logger.py` and `utils/logger.py` into unified `logging_config.py` with backward-compatible re-exports.

**Issue #5: Unused Code** - Removed unused `SystemMonitorWidget` class from widgets.py (actual implementation was in `system_monitor.py`).

### New File Structure
```
src/
‚îú‚îÄ‚îÄ logging_config.py              # Unified logging (260 lines)
‚îú‚îÄ‚îÄ vocabulary/                     # Package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ vocabulary_extractor.py    # Moved + improved (360 lines)
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ text_utils.py              # Shared text utilities (55 lines)
‚îÇ   ‚îî‚îÄ‚îÄ logger.py                  # Re-exports from logging_config
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îú‚îÄ‚îÄ workflow_orchestrator.py   # Workflow logic (180 lines)
‚îÇ   ‚îî‚îÄ‚îÄ queue_message_handler.py   # UI-only routing (210 lines)
```

### Test Results
‚úÖ 55/55 tests PASSED
- Character Sanitization: 22 tests
- Raw Text Extraction: 24 tests
- Progressive Summarization: 4 tests
- Vocabulary Extraction: 5 tests

### File Size Compliance (Target: <300 lines)
- widgets.py: 209 lines ‚úÖ
- queue_message_handler.py: 210 lines ‚úÖ
- workflow_orchestrator.py: 180 lines ‚úÖ
- main_window.py: 295 lines ‚úÖ
- logging_config.py: 260 lines ‚úÖ

### Patterns Established

**Pattern: Workflow Orchestration** - Business logic separated from UI updates. Orchestrator can be unit tested independently.

**Pattern: Unified Logging** - All modules import from `src.logging_config`. Backward compatibility via re-exports.

**Pattern: Shared Utilities** - Pure functions in `src/utils/` package with type hints and docstrings.

### Status
‚úÖ All separation-of-concerns issues resolved
‚úÖ All tests passing (zero regressions)
‚úÖ File sizes compliant
‚úÖ Code duplication eliminated
‚úÖ Logging consolidated

---

## Historical Summary (2025-11-13 to 2025-11-22)

### Major Milestones
1. **CustomTkinter UI Refactor** (2025-11-21): Completed pivot from broken PyQt6 to CustomTkinter dark theme framework, resolving DLL load errors and button visibility issues. Application achieved stable, responsive foundation.

2. **Ollama Backend Migration** (2025-11-17): Successfully migrated from fragile ONNX Runtime (Windows DLL issues, token corruption bugs) to Ollama REST API architecture. Achieved cross-platform stability (Windows/macOS/Linux) with cleaner error handling and runtime model switching.

3. **Critical GUI Crash Fix** (2025-11-17): Resolved application crash after summary generation by adding comprehensive error handling and thread-safe GUI updates. Summaries now display reliably; errors shown gracefully.

4. **UI Polish & Tooltips** (2025-11-22): Fixed tooltip flickering with 500ms delay strategy, darkened menu colors to #404040, standardized quadrant layout (Row 0 labels / Row 1+ content), added smart tooltip positioning fallbacks.

### Technology Stack
- **UI Framework:** CustomTkinter (dark theme, cross-platform)
- **Model Backend:** Ollama (REST API, any HuggingFace model)
- **Concurrency:** ThreadPoolExecutor for I/O-bound operations
- **Configuration:** config.py with DEBUG mode support
- **Logging:** Comprehensive debug logging with CLAUDE.md compliance

### Phase Completion Status
- ‚úÖ **Phase 1:** Document pre-processing (PDF/TXT/RTF extraction, OCR detection, text cleaning)
- ‚úÖ **Phase 2 (Partial):** Desktop UI (CustomTkinter, responsive layout), AI Integration (Ollama), Phase 2.7 (Model formatting), Phase 2.5 (Parallel foundation), Phase 2.6 (System monitor)
- ‚è≥ **Phase 2.2, 2.4:** Document prioritization, License server (post-v1.0)
- üìã **Phase 3+:** Advanced features

### Key Technical Achievements
- Model-agnostic prompt wrapping (Phase 2.7)
- Resource-aware parallel processing formula (Phase 2.5)
- Real-time system monitoring with custom thresholds (Phase 2.6)
- Thread-safe GUI updates with comprehensive error handling
- User preference persistence (settings saved across sessions)
- Professional dark theme with keyboard shortcuts

---

## Session 5 - Code Pattern Refinement: Variable Reuse + Comprehensive Logging (2025-11-25)

**Objective:** Revert Session 4's descriptive variable naming pattern to a more Pythonic approach: variable reassignment with comprehensive logging for observability.

### Rationale for Change
Session 4 introduced explicit `del` statements to manage memory for large files (100MB-500MB). However, this approach was un-Pythonic. Python's garbage collection handles automatic memory cleanup. Better observability comes from comprehensive logging, not variable names.

### Key Changes

**1. CharacterSanitizer.sanitize()** - Reverted from 6 descriptive variables to single `text` variable. Added comprehensive logging for all 6 stages with execution tracking, performance timing, text metrics, and error details. Removed all `del` statements.

**2. RawTextExtractor._normalize_text()** - Reverted from 4 descriptive variables to single `text` variable. Added comprehensive logging for all 4 stages with same pattern as CharacterSanitizer. Removed all `del` statements.

**3. PROJECT_OVERVIEW.md Section 12** - Completely rewrote "Code Patterns & Conventions" to document logging pattern instead of variable naming pattern.

### Testing Results
‚úÖ All 50 core tests PASSED (24 RawTextExtractor + 22 CharacterSanitizer + 4 ProgressiveSummarizer)
- No behavioral changes; all functionality preserved
- Logging enhancements are non-breaking improvements

### Benefits
1. More Pythonic (trusts Python's garbage collection)
2. Simpler code (no try-except blocks for NameError)
3. Better observability (comprehensive logging shows what happened at each stage)
4. Performance insights (timing data for each stage)
5. Debugging support (success/failure logs with error details)
6. Consistent with Python idioms (variable reassignment is standard pattern)

### Bug Fix: Queue Message Handler Attribute Name (Post-Session 5)

**Issue:** Application ran but file processing silently failed with error:
```
[QUEUE HANDLER] Error handling file_processed: '_tkinter.tkapp' object has no attribute 'processing_results'
```

**Root Cause:** Naming inconsistency. The main_window.py defines `self.processed_results`, but queue_message_handler.py was trying to access `self.main_window.processing_results`.

**Fix:** Changed line 48 in src/ui/queue_message_handler.py from `self.main_window.processing_results.append(data)` to `self.main_window.processed_results.append(data)`

**Impact:** File processing results now append correctly, file table updates display properly, no more silent failures.

---

## Session 6 - UI Bug Fixes & Vocabulary Workflow Integration (2025-11-26)

**Features:** Three UI bug fixes, vocabulary extraction workflow integration, spaCy model auto-download, environment path resolution

### Summary
Fixed three UI bugs discovered during manual testing: file size rounding inconsistency, model dropdown selection not persisting, and missing vocabulary extraction workflow. Implemented asynchronous vocabulary extraction with worker thread, graceful fallback for missing config files, and automatic spaCy model download. Resolved critical subprocess PATH issue using `sys.executable` for correct virtual environment targeting.

### Problems Addressed

**Bug #1: File Size Rounding Inconsistency** - Unified all units to round to nearest integer using `round(size)` regardless of unit.

**Bug #2: Model Dropdown Selection Not Working** - Implemented preference preservation logic so selected model doesn't reset on refresh.

**Bug #3: Vocabulary Extraction Workflow Missing** - Multiple issues:
1. Widget reference bug (code called wrong widget method)
2. spaCy model missing (auto-download implemented)
3. Subprocess PATH issue (fixed using `sys.executable`)

### Work Completed

**Part 1:** Fixed file size formatting and model selection in `src/ui/widgets.py`

**Part 2:** Vocabulary workflow integration across multiple files:
- Added `VocabularyWorker` class to `src/ui/workers.py`
- Fixed queue message handler widget references
- Added `_combine_documents()` helper to main_window.py
- Made VocabularyExtractor config files optional

**Part 3:** spaCy model auto-download with correct subprocess targeting
- Initial implementation used `python` command (wrong - might resolve to system Python)
- Fixed to use `sys.executable` (correct - guarantees venv Python)
- Switched from `spacy download` CLI to pip install for reliability
- Added 300-second timeout for download

### Technical Insight: Virtual Environments & Package Storage

**Key Learning:** When spawning subprocesses from a virtual environment:
- `python` command might resolve to system Python, not venv Python
- Packages install to whichever Python executes the install command
- **Solution:** Use `sys.executable` to guarantee correct Python interpreter

### Files Modified
- `src/ui/widgets.py` - File size fix + model selection preservation
- `src/ui/workers.py` - Added VocabularyWorker class
- `src/ui/queue_message_handler.py` - Fixed widget reference + vocab workflow
- `src/ui/main_window.py` - Added `_combine_documents()` helper
- `src/vocabulary_extractor.py` - Config optional + auto-download

### Git Commits
1. `225aa70` - fix: Correct widget reference in vocabulary workflow integration
2. `99fdace` - fix: Add spaCy model auto-download for vocabulary extraction
3. `9de7cb5` - fix: Use correct Python executable path for spaCy model download

### Status
‚úÖ File size rounding consistent
‚úÖ Model dropdown selection preserves user choice
‚úÖ Vocabulary workflow integration complete
‚úÖ spaCy model auto-downloads to correct venv
‚è≥ Pending user testing

---

## Current Project Status

**Application State:** Bug fixes complete; vocabulary workflow integrated; awaiting user testing
**Last Updated:** 2025-11-26 (Session 6 - UI Bug Fixes & Vocabulary Workflow)
**Total Lines of Developed Code:** ~3,600 across all modules
**Code Quality:** All tests passing; comprehensive error handling; debug logging per CLAUDE.md
**Next Priorities:**
1. User testing of vocabulary extraction workflow (next session)
2. Debug any environment/path issues that arise
3. Move to feature development if bugs are resolved
